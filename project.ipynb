{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b632110e",
   "metadata": {},
   "source": [
    "- Dataset? Training data?\n",
    "- Bert or torch from scratch?\n",
    "- which training methods exactly?\n",
    "- paper recommendations?\n",
    "- keywords fo googling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ddc00",
   "metadata": {},
   "source": [
    "## Dataset ideas\n",
    "\n",
    "- https://huggingface.co/datasets/fancyzhx/yelp_polarity\n",
    "- https://huggingface.co/datasets/knowledgator/Scientific-text-classification\n",
    "- https://huggingface.co/datasets/SetFit/mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5090484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertForSequenceClassification, BertTokenizer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m model = BertForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     logging,\n\u001b[32m     51\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m define_import_structure\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\transformers\\utils\\__init__.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     ClassAttrs,\n\u001b[32m     26\u001b[39m     ClassDocstring,\n\u001b[32m     27\u001b[39m     ImageProcessorArgs,\n\u001b[32m     28\u001b[39m     ModelArgs,\n\u001b[32m     29\u001b[39m     ModelOutputArgs,\n\u001b[32m     30\u001b[39m     auto_class_docstring,\n\u001b[32m     31\u001b[39m     auto_docstring,\n\u001b[32m     32\u001b[39m     get_args_doc_from_source,\n\u001b[32m     33\u001b[39m     parse_docstring,\n\u001b[32m     34\u001b[39m     set_min_indent,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py:30\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     MODELS_TO_PIPELINE,\n\u001b[32m     26\u001b[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[32m     27\u001b[39m     PT_SAMPLE_DOCSTRINGS,\n\u001b[32m     28\u001b[39m     _prepare_output_docstrings,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[32m     33\u001b[39m PATH_TO_TRANSFORMERS = Path(\u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m).resolve() / \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m AUTODOC_FILES = [\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodeling_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_extractor_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\transformers\\utils\\generic.py:53\u001b[39m\n\u001b[32m     49\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# required for @can_return_tuple decorator to work with torchdynamo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcached_property\u001b[39;00m(\u001b[38;5;28mproperty\u001b[39m):\n\u001b[32m     57\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m    Descriptor that mimics @property but caches output in member variable.\u001b[39;00m\n\u001b[32m     59\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m \u001b[33;03m    Built-in in functools from Python 3.8.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\torch\\__init__.py:270\u001b[39m\n\u001b[32m    266\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    268\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in path/nvidia/lib_folder/lib or path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\torch\\__init__.py:246\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    244\u001b[39m is_loaded = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     res = \u001b[43mkernel32\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     last_error = ctypes.get_last_error()\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error != \u001b[32m126\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Sample text for training.\"\n",
    "label = 1  # Assuming positive sentiment\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, labels=torch.tensor([label]))\n",
    "\n",
    "loss_func = outputs.loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_func.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384111f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fee61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pip install -q transformers datasets accelerate torch==2.* sentencepiece\n",
    "import os, math, random, torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BertTokenizer, BertForMaskedLM, PreTrainedTokenizerBase, SequenceFeatureExtractor, DataCollatorWithPadding\n",
    "from torch.utils.data import default_collate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df063e",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9f8e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target', 'rationale', 'task', 'type'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"kaist-ai/CoT-Collection\", trust_remote_code=True, split='train[:1000]')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fb374d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Article: Phytochemistry is a branch of plant biochemistry primarily concerned with the chemical substances produced by plants during secondary metabolism. Some of these compounds are toxins such as the alkaloid coniine from hemlock. Others, such as the essential oils peppermint oil and lemon oil are useful for their aroma, as flavourings and spices (e.g., capsaicin), and in medicine as pharmaceuticals as in opium from opium poppies. Many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. Others are simple derivatives of botanical natural products. For example, the pain killer aspirin is the acetyl ester of salicylic acid, originally isolated from the bark of willow trees, and a wide range of opiate painkillers like heroin are obtained by chemical modification of morphine obtained from the opium poppy. Popular stimulants come from plants, such as caffeine from coffee, tea and chocolate, and nicotine from tobacco. Most alcoholic beverages come from fermentation of carbohydrate-rich plant products such as barley (beer), rice (sake) and grapes (wine).\\n\\nNow answer this question: Where do some medicines and recreational drugs come from?',\n",
       " 'target': 'from plants',\n",
       " 'rationale': 'The article states that many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. These are some examples of the medicines found in plants mentioned by the author. Thus it can be stated with certainty that some medicines do indeed come from plants.\\n\\nTherefore, \"from plants\" is the correct answer option to this question based on the context provided.\"',\n",
       " 'task': 'squad_v1',\n",
       " 'type': 'CoT'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280b85f",
   "metadata": {},
   "source": [
    "# CoT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59dccd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ckpt = \"bert-base-uncased\"     # swap for domain/multilingual BERT as needed\n",
    "tok: BertTokenizer = BertTokenizer.from_pretrained(ckpt)\n",
    "model = BertForMaskedLM.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24df519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token: 101\n",
      "sep_token: 102\n",
      "mask_token: 103\n"
     ]
    }
   ],
   "source": [
    "# Special tokens\n",
    "print(\"cls_token:\", tok.cls_token_id)\n",
    "print(\"sep_token:\", tok.sep_token_id)\n",
    "print(\"mask_token:\", tok.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b36662f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['some sample text for training and for testing']\n",
      "input: \t tensor([[2070, 7099, 3793, 2005, 2731,  103, 2005, 5604]]) ['some sample text for training [MASK] for testing']\n",
      "output:  tensor([[1998, 2440, 2817, 2005, 5604, 1998, 2005, 5604]]) ['and full study for testing and for testing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "text = \"Some sample text for training and for testing\"\n",
    "input = tok(text, return_tensors='pt', add_special_tokens=False)\n",
    "print(tok.batch_decode(input['input_ids']))\n",
    "input['attention_mask'][0][-3] = 0\n",
    "input['input_ids'][0][-3] = tok.mask_token_id\n",
    "print('input: \\t', input['input_ids'], tok.batch_decode(input['input_ids']))\n",
    "logits = model.forward(**input).logits\n",
    "output = torch.argmax(logits, dim=2)\n",
    "print('output: ', output, tok.batch_decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494ed89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GeneratorTrainer:\n",
    "    model: BertForMaskedLM\n",
    "    ds: Dataset\n",
    "    tokenizer: BertTokenizer\n",
    "    teacher_forcing_percentage: float = 0.8\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "        self.cls_token_tensor = torch.tensor([[self.tokenizer.cls_token_id]])\n",
    "\n",
    "\n",
    "    def tokenize(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        questions = self.tokenizer(features[\"source\"], add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "        answers = self.tokenizer(features[\"rationale\"], add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "        return questions['input_ids'], answers['input_ids']\n",
    "\n",
    "\n",
    "    def generate(self, questions: torch.Tensor, answers: torch.Tensor|None=None, max_length=200):\n",
    "        generated_answers = []\n",
    "        logits = []\n",
    "        batch_size = questions.shape[0]\n",
    "        cls_token_tensor = self.tokenizer.cls_token_id * torch.ones((batch_size, 1))\n",
    "        mask_token_tensor = self.tokenizer.mask_token_id * torch.ones((batch_size, 1))\n",
    "        sep_token_tensor = self.tokenizer.sep_token_id * torch.ones((batch_size, 1))\n",
    "        for i in range(answers.shape[1] if answers is not None else max_length):\n",
    "            use_teacher_forcing = (answers is not None) and (i < len(answers)) and (random.random() < self.teacher_forcing_percentage)\n",
    "            prefix = answers[:, :i] if use_teacher_forcing else (torch.stack(generated_answers, axis=1) if len(generated_answers)>0 else torch.zeros((batch_size, 0)))\n",
    "\n",
    "            #print(*(x.size() for x in (cls_token_tensor, questions, prefix, mask_token_tensor, sep_token_tensor)))\n",
    "            inp = torch.concat((cls_token_tensor, questions, prefix, mask_token_tensor, sep_token_tensor), dim=1)\n",
    "            mask_pos = -2\n",
    "            attention_mask = torch.ones(inp.shape)\n",
    "            attention_mask[:,mask_pos] = 0\n",
    "            token_type_ids = torch.concat((torch.zeros((batch_size, questions.shape[1]+1)), torch.ones((batch_size, prefix.shape[1]+2))), dim=1)\n",
    "\n",
    "            #print(*(x.size() for x in (inp.int(), token_type_ids.int(), attention_mask.int())))\n",
    "            generated = self.model.forward(input_ids=inp.int(), token_type_ids=token_type_ids.int(), attention_mask=attention_mask.int())\n",
    "\n",
    "            generated_answers.append(torch.argmax(generated.logits[:,mask_pos], dim=-1))\n",
    "            logits.append(generated.logits[:,mask_pos])\n",
    "        #print(generated_answers, logits)\n",
    "        #print(torch.stack(generated_answers).shape)\n",
    "        print(len(generated_answers))\n",
    "        print(torch.stack(generated_answers).size(), torch.stack(logits).size())\n",
    "        return torch.stack(generated_answers, dim=1), torch.stack(logits, dim=1)\n",
    "\n",
    "    def train(self, episodes, batch_size=16):\n",
    "        for episode in range(episodes):\n",
    "            i_samples = np.random.randint(0, len(self.ds), batch_size)\n",
    "            samples = self.ds.select(i_samples)\n",
    "            answers, questions = self.tokenize(samples)\n",
    "            generated_answers, logits = self.generate(questions, answers)\n",
    "\n",
    "            loss = self.loss_func(logits, answers)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            print(loss, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557de5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "generator_trainer = GeneratorTrainer(model, raw_dataset, tok)\n",
    "generator_trainer.train(episodes=10, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428fb56d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_with_mlm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mQuestion: What is 7 + 5?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mReasoning:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_with_mlm\u001b[49m(prompt, max_new_tokens=\u001b[32m40\u001b[39m, stop_strings=[\u001b[33m\"\u001b[39m\u001b[33mAnswer:\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_with_mlm' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"Question: What is 7 + 5?\\nReasoning:\"\n",
    "print(generate_with_mlm(prompt, max_new_tokens=40, stop_strings=[\"Answer:\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
