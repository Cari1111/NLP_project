{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b632110e",
   "metadata": {},
   "source": [
    "- Dataset? Training data?\n",
    "- Bert or torch from scratch?\n",
    "- which training methods exactly?\n",
    "- paper recommendations?\n",
    "- keywords fo googling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e09fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\csms\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1094,  0.2831, -0.2219,  ..., -0.4116,  0.2604,  0.4303],\n",
      "         [ 0.3717,  0.0461,  0.3014,  ..., -0.2603, -0.2263,  0.0782],\n",
      "         [-0.2072,  0.3525, -0.2668,  ..., -0.0065, -0.6920, -0.0067],\n",
      "         ...,\n",
      "         [ 0.5834,  0.4004,  0.4881,  ..., -0.1181,  0.2630, -0.1225],\n",
      "         [ 0.0716, -0.2377, -0.3874,  ...,  0.2981,  0.0436, -0.6352],\n",
      "         [ 0.8747,  0.4112, -0.3423,  ...,  0.2614, -0.4867, -0.2898]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"BERT embeddings are fascinating.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "word_embeddings = outputs.last_hidden_state\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116e1eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 14324,  7861,  8270,  4667,  2015,  2024, 17160,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb184784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [unused0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "original_text = \"Long text for summarization...\"\n",
    "inputs = tokenizer(original_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "summary_logits = model(**inputs).logits\n",
    "summary = tokenizer.decode(torch.argmax(summary_logits, dim=1))\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5090484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\csms\\miniconda3\\envs\\NLP-Lecture\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Sample text for training.\"\n",
    "label = 1  # Assuming positive sentiment\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, labels=torch.tensor([label]))\n",
    "\n",
    "loss = outputs.loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384111f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
